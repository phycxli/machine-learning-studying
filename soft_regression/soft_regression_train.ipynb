{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Regression\n",
    "\n",
    "主要用于处理多分类问题，其中任意两个类是线性可分的。\n",
    "\n",
    "假设有$N$个训练样本$\\{(X_1,y_1),(X_2,y_2),\\cdots,(X_N,y_N)\\}$，对于soft regression算法，输入特征为$X_i\\in\\mathcal{R}^{n+1}$，类标记为:$y_i\\in\\{0,1,\\cdots,k\\}$.\n",
    "\n",
    "假设每一个样本估计所属的类概率为$P(y=j|X)$，假设函数为:\n",
    "\\begin{equation}\n",
    "h_\\theta(X_i)=\\begin{bmatrix}P(y_i=1|X_i;\\theta)\\\\P(y_i=2|X_i;\\theta)\\\\\\vdots\\\\P(y_i=k|X_i;\\theta)\\end{bmatrix}=\\frac{1}{\\sum_{j=1}^{k}e^{\\theta_j^T X_i}}\\begin{bmatrix}e^{\\theta_1^T X_i}\\\\e^{\\theta_2^T X_i}\\\\\\vdots\\\\e^{\\theta_k^T X_i}\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "其中$\\theta$表示的向量，且$\\theta_i\\in\\mathcal{R}^{n+1}$。每一个样本所属类的概率为$P(y_i=j|X_i;\\theta)=\\frac{e^{\\theta_j^T X_i}}{\\sum_{l=1}^{k}e^{\\theta_l^T X_i}}$\n",
    "\n",
    "损失函数用交叉熵$l(\\theta)=-\\frac{1}{N}[\\sum_{i=1}^N\\sum_{j=1}^kI(y_i=j)\\ln\\frac{e^{\\theta_j^T X_i}}{\\sum_{l=1}^{k}e^{\\theta_l^T X_i}}]$，$I(y_i=j)$表示指示函数\n",
    "\n",
    "损失函数的梯度表达式为:$\\nabla_{\\theta_j} l(\\theta)=-\\frac{1}{N}\\sum_{i=1}^{N}[X_i\\cdot (I(y_i=j)-P(y_i=j|X_i;\\theta))]$\n",
    "\n",
    "梯度下降法更新公式为:$\\theta_j=\\theta_j-\\alpha\\nabla_{\\theta_j}l(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:UTF-8\n",
    "import numpy as np\n",
    "import os\n",
    "cwd=os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#首先实现对于一个np.mat矩阵，不同元素值计数\n",
    "def count_diff(mats):\n",
    "    '''实现矩阵不同元素的计数\n",
    "    input:  mats(mat)输入矩阵\n",
    "    output: elements(int)不同元素的个数\n",
    "    '''\n",
    "    element,count=np.unique(np.array(mats.T),return_counts=True)\n",
    "    return len(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算损失函数\n",
    "def loss(expo,labels):\n",
    "    '''计算损失函数值\n",
    "    input:  expo(mat):概率exp因子\n",
    "            labels(mat):标签值\n",
    "    '''\n",
    "    num=np.shape(expo)[0]\n",
    "    loss_total=0\n",
    "    for i in range(num):\n",
    "        if expo[i, labels[i, 0]] / np.sum(expo[i, :]) > 0:\n",
    "            loss_total -= np.log(expo[i, labels[i, 0]] / np.sum(expo[i, :]))\n",
    "        else:\n",
    "            loss_total -= 0\n",
    "    return loss_total / num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#梯度下降求解模型参数\n",
    "def gradascent(features,labels,k,epochs,alpha):\n",
    "    '''利用梯度下降法训练softmax模型\n",
    "    input:  features(mat):特征\n",
    "            labels(mat):标签\n",
    "            epochs(int):迭代次数\n",
    "            alpha(float):学习率\n",
    "    output: weights(mat):权重\n",
    "    '''\n",
    "    num,n=np.shape(features)    #m样本个数，n特征个数\n",
    "    #k=count_diff(labels)        #labels里有k个类\n",
    "    weights=np.mat(np.ones((n,k)))#初始权重\n",
    "    i=0\n",
    "    while i<=epochs:\n",
    "        expo=np.exp(features*weights)\n",
    "        if i%500==0:\n",
    "            print(\"\\t-----iter:\",i,\",loss:\",loss(expo,labels))\n",
    "        expo_sum=-expo.sum(axis=1)\n",
    "        expo_sum=expo_sum.repeat(k,axis=1)    #求和求完后变成(num,1)的列矩阵，所以需要复制k次\n",
    "        expo=expo/expo_sum\n",
    "        for j in range(num):\n",
    "            expo[j,labels[j,0]]+=1\n",
    "        weights=weights+(alpha/num)*features.T*expo\n",
    "        i+=1\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.loading\n",
      "2.training\n",
      "\t-----iter: 0 ,loss: 1.3862943611198923\n",
      "\t-----iter: 500 ,loss: 0.9954868729037445\n",
      "\t-----iter: 1000 ,loss: 0.6856022967575431\n",
      "\t-----iter: 1500 ,loss: 0.5920097490486217\n",
      "\t-----iter: 2000 ,loss: 0.5132045267831212\n",
      "\t-----iter: 2500 ,loss: 0.44883718511917964\n",
      "\t-----iter: 3000 ,loss: 0.39601515963710143\n",
      "\t-----iter: 3500 ,loss: 0.34996176453553707\n",
      "\t-----iter: 4000 ,loss: 0.3077926574058134\n",
      "\t-----iter: 4500 ,loss: 0.26841045522817003\n",
      "\t-----iter: 5000 ,loss: 0.23129403395214138\n",
      "\t-----iter: 5500 ,loss: 0.1952081854877084\n",
      "\t-----iter: 6000 ,loss: 0.1565769620321188\n",
      "\t-----iter: 6500 ,loss: 0.09624605082832434\n",
      "\t-----iter: 7000 ,loss: 0.08658249945375969\n",
      "\t-----iter: 7500 ,loss: 0.08485014399583188\n",
      "\t-----iter: 8000 ,loss: 0.08331988013807425\n",
      "\t-----iter: 8500 ,loss: 0.08194251483383072\n",
      "\t-----iter: 9000 ,loss: 0.08068611821699209\n",
      "\t-----iter: 9500 ,loss: 0.07952840681680758\n",
      "\t-----iter: 10000 ,loss: 0.07845305190209749\n",
      "3.saving\n"
     ]
    }
   ],
   "source": [
    "#导入数据\n",
    "def load_data(filepath):\n",
    "    '''\n",
    "    input:  filepath(str)训练集文件路径\n",
    "    output: features(mat)特征\n",
    "            labels(mat)标签\n",
    "    '''\n",
    "    file=open(filepath)\n",
    "    features=[]\n",
    "    labels=[]\n",
    "    for row in file.readlines():\n",
    "        features_temp=[]\n",
    "        features_temp.append(1)\n",
    "        row=row.strip().split('\\t')\n",
    "        for i in range(len(row)-1):\n",
    "            features_temp.append(float(row[i]))\n",
    "        labels.append(int(row[-1]))\n",
    "        features.append(features_temp)\n",
    "    file.close()\n",
    "    return np.mat(features),np.mat(labels).T,len(set(labels))\n",
    "\n",
    "#保存数据\n",
    "def save_model(filepath,weights):\n",
    "    '''保存最后的模型\n",
    "    input:  filepath(str):保存的路径\n",
    "            weights(mat):保存的权重\n",
    "    '''\n",
    "    file=open(filepath,'w')\n",
    "    n,k=np.shape(weights)\n",
    "    for i in range(n):\n",
    "        weights_temp=[]\n",
    "        for j in range(k):\n",
    "            weights_temp.append(str(weights[i,j]))   #实际上这里就是单纯做了一个格式转换，把矩阵里的float转化成str，方便用open()方法存储\n",
    "        file.write('\\t'.join(weights_temp)+'\\n')\n",
    "    file.close()\n",
    "\n",
    "#训练数据主函数\n",
    "if __name__=='__main__':\n",
    "    filename=os.path.join(cwd,'SoftInput.txt')\n",
    "    print('1.loading')\n",
    "    features,labels,k=load_data(filename)\n",
    "    print('2.training')\n",
    "    weights=gradascent(features=features,labels=labels,k=k,epochs=10000,alpha=0.4)\n",
    "    print('3.saving')\n",
    "    savename=os.path.join(cwd,'model')\n",
    "    save_model(savename,weights=weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

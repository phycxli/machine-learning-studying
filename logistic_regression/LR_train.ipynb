{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Sigmoid函数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于输入向量X，属于正例的概率$P(y=1|X,W,b)=\\sigma(WX+b)=\\frac{1}{1+e^{-(WX+b)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类别$y$的概率为$P(y|X,W,b)=\\sigma(WX+b)^y(1-\\sigma(WX+b))^{1-y}$，两点分布"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "似然函数$L_{W,b}=\\prod_{i=1}^{N}h_{W,b}(X_i)^y_i(1-h_{W,b}(X_i))^{1-y_i}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设函数$h_{W,b}(X_i)=\\sigma(WX_i+b)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "负对数似然函数设置为损失函数:$l_{W,b}=-\\frac{1}{N}\\sum_{i=1}^N [y_i log(h_{W,b}(X_i))+(1-y_i) log(1-h_{W,b}(X_i))]$\n",
    "\n",
    "只需要求极小值问题$\\min_{W,b}l_{W,b}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.梯度下降"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化问题$minf(w)$，梯度下降法的过程\n",
    "\n",
    "1.随机选择一个初始点$w_0$\n",
    "\n",
    "2.重复以下过程\n",
    "\n",
    "决定梯度下降的方向：$d_i=-\\frac{\\partial}{\\partial w}f(w)|_{w_i}$\n",
    "\n",
    "选择步长$\\alpha$\n",
    "\n",
    "更新：$w_{i+1}=w_i+\\alpha\\cdot d_i$\n",
    "\n",
    "3.直到满足终止条件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#损失函数\n",
    "def loss(h,label):\n",
    "    '''计算当前损失函数值\n",
    "    input:  h(mat):预测值\n",
    "            label(mat):实际值\n",
    "    output: loss/m(float):损失率\n",
    "    '''\n",
    "    num=np.shape(h)[0]  #num表示样本数量(x_i,y_i)，i=1,2,..,num的个数\n",
    "    sum_loss=0.0\n",
    "    for i in range(num):\n",
    "        if h[i,0]>0 and (1-h[i,0])>0:\n",
    "            sum_loss=sum_loss-(label[i,0]*np.log(h[i,0])+(1-label[i,0])*np.log(1-h[i,0]))\n",
    "        else:\n",
    "            sum_loss=sum_loss-0\n",
    "        return sum_loss/num\n",
    "    \n",
    "#梯度下降\n",
    "def lr_train(feature,label,epoch,alpha):\n",
    "    '''利用梯度下降法训练LR模型\n",
    "    input:  feature(mat)特征\n",
    "            label(mat)标签\n",
    "            epoch(int)最大迭代次数\n",
    "            alpha(float)学习率\n",
    "    '''\n",
    "    n=np.shape(feature)[1]  #特征个数\n",
    "    w=np.mat(np.ones((n,1)))#初始化权重 np.ones(m,n)表示建立m×n的向量，np.mat表示矩阵化\n",
    "    i=0\n",
    "    while i<=epoch:\n",
    "        i+=1\n",
    "        h=sig(feature*w)\n",
    "        err=label-h #表示标签y_i与lr模型值的差，可以理解成误差\n",
    "        if i%100==0:\n",
    "            print(\"\\t---------iter=\"+str(i)+\", train error rate=\"+str(loss(h,label)))\n",
    "            w = w + alpha * feature.T * err\n",
    "        return w"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.训练数据\n",
    "\n",
    "首先定义导入训练数据的函数load_data和保存模型的函数save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    '''\n",
    "    input:  file_name(str)训练数据的路径地址\n",
    "    output: features(mat)特征\n",
    "            labels(mat)标签\n",
    "    '''\n",
    "    file=open(file_name)\n",
    "    features=[]\n",
    "    labels=[]\n",
    "    for row in file.readlines():\n",
    "        features_temp=[]\n",
    "        labels_temp=[]\n",
    "        rows=row.strip().split('\\t')\n",
    "        features_temp.append(1)\n",
    "        for i in range(len(rows)-1):\n",
    "            features_temp.append(float(rows[i]))\n",
    "        labels_temp.append(float(rows[-1]))\n",
    "        features.append(features_temp)\n",
    "        labels.append(labels_temp)\n",
    "    file.close()\n",
    "    return np.mat(features),np.mat(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(file_name,w):\n",
    "    '''保存模型参数\n",
    "    input:  file_name(string)文件名\n",
    "            w(mat)LR模型的权重参数\n",
    "    '''\n",
    "    num=np.shape(w)[0]  #样本数量\n",
    "    file_w=open(file_name,'w')\n",
    "    w_array=[]\n",
    "    for i in range(num):\n",
    "        w_array.append(str(w[i,0]))\n",
    "    file_w.write('\\t'.join(w_array))\n",
    "    file_w.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "training\n",
      "saving model\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    print('loading data')\n",
    "    feature,label=load_data('data.txt')\n",
    "    print('training')\n",
    "    w=lr_train(feature=feature,label=label,epoch=100,alpha=0.01)\n",
    "    print('saving model')\n",
    "    save_model('weights',w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
